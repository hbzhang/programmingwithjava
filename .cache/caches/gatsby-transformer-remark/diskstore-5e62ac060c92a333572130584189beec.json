{"expireTime":9007200813457781000,"key":"transformer-remark-markdown-html-ff56c2e4d493a82f77a53e9d7e497a35-gatsby-remark-imagesgatsby-remark-snippetsgatsby-remark-autolink-headersgatsby-remark-code-titlesgatsby-remark-prismjsgatsby-remark-external-links-","val":"<h2 id=\"basic-operations\"><a href=\"#basic-operations\" aria-label=\"basic operations permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Basic Operations</h2>\n<h3 id=\"tensorflow-constant\"><a href=\"#tensorflow-constant\" aria-label=\"tensorflow constant permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensorflow constant</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Build a graph.\na = tf.constant(5.0)\nb = tf.constant(6.0)\nc = a * b\n\n# Launch the graph in a session.\nsess = tf.Session()\n\n# Evaluate the tensor `c`.\nprint(sess.run(c))</code></pre></div>\n<h3 id=\"tensorflow-variable\"><a href=\"#tensorflow-variable\" aria-label=\"tensorflow variable permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensorflow variable</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">https://www.tensorflow.org/api_docs/python/tf/Variable\n\nimport tensorflow as tf\n\n# Create a variable.\nw = tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)\n\n# Use the variable in the graph like any Tensor.\ny = tf.matmul(w, ...another variable or tensor...)\n\n# The overloaded operators are available too.\nz = tf.sigmoid(w + y)\n\n# Assign a new value to the variable with `assign()` or a related method.\nw.assign(w + 1.0)\nw.assign_add(1.0)</code></pre></div>\n<h3 id=\"tensorflow-variable-1\"><a href=\"#tensorflow-variable-1\" aria-label=\"tensorflow variable 1 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensorflow variable</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Using python state\nx = tf.zeros([10, 10])\nx += 2  # This is equivalent to x = x + 2, which does not mutate the original\n        # value of x\nprint(x)</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">v = tf.Variable(1.0)\nassert v.numpy() == 1.0\n\n# Re-assign the value\nv.assign(3.0)\nassert v.numpy() == 3.0\n\n# Use `v` in a TensorFlow operation like tf.square() and reassign\nv.assign(tf.square(v))\nassert v.numpy() == 9.0</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">class MyDenseLayer(tf.keras.layers.Layer):\n  def __init__(self, num_outputs):\n    super(MyDenseLayer, self).__init__()\n    self.num_outputs = num_outputs\n\n  def build(self, input_shape):\n    self.kernel = self.add_variable(&quot;kernel&quot;,\n                                    shape=[int(input_shape[-1]),\n                                           self.num_outputs])\n\n  def call(self, input):\n    return tf.matmul(input, self.kernel)\n\nlayer = MyDenseLayer(10)\nprint(layer(tf.zeros([10, 5])))\nprint(layer.trainable_variables)</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">class MyDenseLayer(tf.keras.layers.Layer):\n  def __init__(self, num_outputs):\n    super(MyDenseLayer, self).__init__()\n    self.num_outputs = num_outputs\n    \n  def build(self, input_shape):\n    self.kernel = self.add_variable(&quot;kernel&quot;, \n                                    shape=[int(input_shape[-1]), \n                                           self.num_outputs])\n    \n  def call(self, input):\n    return tf.matmul(input, self.kernel)\n  \nlayer = MyDenseLayer(10)\nprint(layer(tf.zeros([10, 5])))\nprint(layer.trainable_variables)</code></pre></div>\n<h3 id=\"initilize-tensors\"><a href=\"#initilize-tensors\" aria-label=\"initilize tensors permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>initilize tensors</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">(10,10) array with zero elements\n\n# Using python state\nx = tf.zeros([10, 10])\nx += 2  # This is equivalent to x = x + 2, which does not mutate the original\n        # value of x\nprint(x)</code></pre></div>\n<h3 id=\"tensors-basic-operations\"><a href=\"#tensors-basic-operations\" aria-label=\"tensors basic operations permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensors basic operations</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import tensorflow as tf\nimport numpy \n\ntf.enable_eager_execution\n\nprint(tf.add(1, 2))\nprint(tf.add([1, 2], [3, 4]))\nprint(tf.square(5))\nprint(tf.reduce_sum([1, 2, 3]))\nprint(tf.encode_base64(&quot;hello world&quot;))\n\n# Operator overloading is also supported\nprint(tf.square(2) + tf.square(3))\n\nx = tf.matmul([[1]], [[2, 3]])\nprint(x.shape)\nprint(x.dtype)\n\nndarray = np.ones([3, 3])\n\nprint(&quot;TensorFlow operations convert numpy arrays to Tensors automatically&quot;)\ntensor = tf.multiply(ndarray, 42)\nprint(tensor)\n\nprint(&quot;And NumPy operations convert Tensors to numpy arrays automatically&quot;)\nprint(np.add(tensor, 1))\n\nprint(&quot;The .numpy() method explicitly converts a Tensor to a numpy array&quot;)\nprint(tensor.numpy())</code></pre></div>\n<h3 id=\"tensorflow-save-on-gpu\"><a href=\"#tensorflow-save-on-gpu\" aria-label=\"tensorflow save on gpu permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensorflow save on GPU</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">x = tf.random_uniform([3, 3])\n\nprint(&quot;Is there a GPU available: &quot;),\nprint(tf.test.is_gpu_available())\n\nprint(&quot;Is the Tensor on GPU #0:  &quot;),\nprint(x.device.endswith(&#39;GPU:0&#39;))</code></pre></div>\n<h3 id=\"tensorflow-uses-gpu\"><a href=\"#tensorflow-uses-gpu\" aria-label=\"tensorflow uses gpu permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensorflow uses GPU</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import time\n\ndef time_matmul(x):\n  start = time.time()\n  for loop in range(10):\n    tf.matmul(x, x)\n\n  result = time.time()-start\n    \n  print(&quot;10 loops: {:0.2f}ms&quot;.format(1000*result))\n\n\n# Force execution on CPU\nprint(&quot;On CPU:&quot;)\nwith tf.device(&quot;CPU:0&quot;):\n  x = tf.random_uniform([1000, 1000])\n  assert x.device.endswith(&quot;CPU:0&quot;)\n  time_matmul(x)\n\n# Force execution on GPU #0 if available\nif tf.test.is_gpu_available():\n  with tf.device(&quot;GPU:0&quot;): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n    x = tf.random_uniform([1000, 1000])\n    assert x.device.endswith(&quot;GPU:0&quot;)\n    time_matmul(x)</code></pre></div>\n<h3 id=\"automatic-differentation\"><a href=\"#automatic-differentation\" aria-label=\"automatic differentation permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Automatic Differentation</h3>\n<p>x = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as t:\nt.watch(x)\ny = x * x\nz = y * y\ndz<em>dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\ndy</em>dx = t.gradient(y, x)  # 6.0\ndel t  # Drop the reference to the tape</p>\n<h3 id=\"a-small-script-to-show-a-custom-training-using\"><a href=\"#a-small-script-to-show-a-custom-training-using\" aria-label=\"a small script to show a custom training using permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>A small script to show a custom training using</h3>\n<ul>\n<li>class,  assert, plot  </li>\n<li>variable, gradient, and loss</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\n\ntf.enable_eager_execution()\n\n&quot;&quot;&quot;## Variables\n\nTensors in TensorFlow are immutable stateless objects. Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!). To represent this state which needs to change over the course of your computation, you can choose to rely on the fact that Python is a stateful programming language:\n&quot;&quot;&quot;\n\n# Using python state\nx = tf.zeros([10, 10])\nx += 2  # This is equivalent to x = x + 2, which does not mutate the original\n        # value of x\nprint(x)\n\n&quot;&quot;&quot;TensorFlow, however, has stateful operations built in, and these are often more pleasant to use than low-level Python representations of your state. To represent weights in a model, for example, it&#39;s often convenient and efficient to use TensorFlow variables.\n\nA Variable is an object which stores a value and, when used in a TensorFlow computation, will implicitly read from this stored value. There are operations (`tf.assign_sub`, `tf.scatter_update`, etc) which manipulate the value stored in a TensorFlow variable.\n&quot;&quot;&quot;\n\nv = tf.Variable(1.0)\nassert v.numpy() == 1.0\n\n# Re-assign the value\nv.assign(3.0)\nassert v.numpy() == 3.0\n\n# Use `v` in a TensorFlow operation like tf.square() and reassign\nv.assign(tf.square(v))\nassert v.numpy() == 9.0\n\n&quot;&quot;&quot;Computations using Variables are automatically traced when computing gradients. For Variables representing embeddings TensorFlow will do sparse updates by default, which are more computation and memory efficient.\n\nUsing Variables is also a way to quickly let a reader of your code know that this piece of state is mutable.\n\n## Example: Fitting a linear model\n\nLet&#39;s now put the few concepts we have so far ---`Tensor`, `GradientTape`, `Variable` --- to build and train a simple model. This typically involves a few steps:\n\n1. Define the model.\n2. Define a loss function.\n3. Obtain training data.\n4. Run through the training data and use an &quot;optimizer&quot; to adjust the variables to fit the data.\n\nIn this tutorial, we&#39;ll walk through a trivial example of a simple linear model: `f(x) = x * W + b`, which has two variables - `W` and `b`. Furthermore, we&#39;ll synthesize data such that a well trained model would have `W = 3.0` and `b = 2.0`.\n\n### Define the model\n\nLet&#39;s define a simple class to encapsulate the variables and the computation.\n&quot;&quot;&quot;\n\nclass Model(object):\n  def __init__(self):\n    # Initialize variable to (5.0, 0.0)\n    # In practice, these should be initialized to random values.\n    self.W = tf.Variable(5.0)\n    self.b = tf.Variable(0.0)\n\n  def __call__(self, x):\n    return self.W * x + self.b\n\nmodel = Model()\n\nassert model(3.0).numpy() == 15.0\n\n&quot;&quot;&quot;### Define a loss function\n\nA loss function measures how well the output of a model for a given input matches the desired output. Let&#39;s use the standard L2 loss.\n&quot;&quot;&quot;\n\ndef loss(predicted_y, desired_y):\n  return tf.reduce_mean(tf.square(predicted_y - desired_y))\n\n&quot;&quot;&quot;### Obtain training data\n\nLet&#39;s synthesize the training data with some noise.\n&quot;&quot;&quot;\n\nTRUE_W = 3.0\nTRUE_b = 2.0\nNUM_EXAMPLES = 1000\n\ninputs  = tf.random_normal(shape=[NUM_EXAMPLES])\nnoise   = tf.random_normal(shape=[NUM_EXAMPLES])\noutputs = inputs * TRUE_W + TRUE_b + noise\n\n&quot;&quot;&quot;Before we train the model let&#39;s visualize where the model stands right now. We&#39;ll plot the model&#39;s predictions in red and the training data in blue.&quot;&quot;&quot;\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(inputs, outputs, c=&#39;b&#39;)\nplt.scatter(inputs, model(inputs), c=&#39;r&#39;)\nplt.show()\n\nprint(&#39;Current loss: &#39;),\nprint(loss(model(inputs), outputs).numpy())\n\n&quot;&quot;&quot;### Define a training loop\n\nWe now have our network and our training data. Let&#39;s train it, i.e., use the training data to update the model&#39;s variables (`W` and `b`) so that the loss goes down using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent). There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer` implementations. We&#39;d highly recommend using those implementations, but in the spirit of building from first principles, in this particular example we will implement the basic math ourselves.\n&quot;&quot;&quot;\n\ndef train(model, inputs, outputs, learning_rate):\n  with tf.GradientTape() as t:\n    current_loss = loss(model(inputs), outputs)\n  dW, db = t.gradient(current_loss, [model.W, model.b])\n  model.W.assign_sub(learning_rate * dW)\n  model.b.assign_sub(learning_rate * db)\n\n&quot;&quot;&quot;Finally, let&#39;s repeatedly run through the training data and see how `W` and `b` evolve.&quot;&quot;&quot;\n\nmodel = Model()\n\n# Collect the history of W-values and b-values to plot later\nWs, bs = [], []\nepochs = range(10)\nfor epoch in epochs:\n  Ws.append(model.W.numpy())\n  bs.append(model.b.numpy())\n  current_loss = loss(model(inputs), outputs)\n\n  train(model, inputs, outputs, learning_rate=0.1)\n  print(&#39;Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f&#39; %\n        (epoch, Ws[-1], bs[-1], current_loss))\n\n# Let&#39;s plot it all\nplt.plot(epochs, Ws, &#39;r&#39;,\n         epochs, bs, &#39;b&#39;)\nplt.plot([TRUE_W] * len(epochs), &#39;r--&#39;,\n         [TRUE_b] * len(epochs), &#39;b--&#39;)\nplt.legend([&#39;W&#39;, &#39;b&#39;, &#39;true W&#39;, &#39;true_b&#39;])\nplt.show()\n\n&quot;&quot;&quot;## Next Steps\n\nIn this tutorial we covered `Variable`s and built and trained a simple linear model using the TensorFlow primitives discussed so far.\n\nIn theory, this is pretty much all you need to use TensorFlow for your machine learning research.\nIn practice, particularly for neural networks, the higher level APIs like `tf.keras` will be much more convenient since it provides higher level building blocks (called &quot;layers&quot;), utilities to save and restore state, a suite of loss functions, a suite of optimization strategies etc.\n&quot;&quot;&quot;</code></pre></div>"}