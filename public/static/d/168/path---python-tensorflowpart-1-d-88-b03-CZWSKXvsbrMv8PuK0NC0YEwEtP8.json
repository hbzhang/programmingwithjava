{"data":{"site":{"siteMetadata":{"siteUrl":"http://ghost.local:8000","title":"Ghost Docs","description":"Everything you need to know about working with the Ghost professional publishing platform."}},"markdownRemark":{"frontmatter":{"title":"Tensorflow in Python Part I  ","date":"2019-05-23","date_pretty":"23 May, 2019","path":null,"meta_title":"Python Data Structure","meta_description":"This tutorial is used for going through python data structure","image":null,"next":null,"sidebar":null,"toc":null,"keywords":["setup","production","server","ubuntu"]},"html":"<h2 id=\"basic-operations\"><a href=\"#basic-operations\" aria-label=\"basic operations permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Basic Operations</h2>\n<h3 id=\"initilize-tensors\"><a href=\"#initilize-tensors\" aria-label=\"initilize tensors permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>initilize tensors</h3>\n<p>(10,10) array with zero elements</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># Using python state\nx = tf.zeros([10, 10])\nx += 2  # This is equivalent to x = x + 2, which does not mutate the original\n        # value of x\nprint(x)</code></pre></div>\n<h3 id=\"tensors-basic-operations\"><a href=\"#tensors-basic-operations\" aria-label=\"tensors basic operations permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensors basic operations</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import tensorflow as tf\nimport numpy \n\ntf.enable_eager_execution\n\nprint(tf.add(1, 2))\nprint(tf.add([1, 2], [3, 4]))\nprint(tf.square(5))\nprint(tf.reduce_sum([1, 2, 3]))\nprint(tf.encode_base64(&quot;hello world&quot;))\n\n# Operator overloading is also supported\nprint(tf.square(2) + tf.square(3))\n\nx = tf.matmul([[1]], [[2, 3]])\nprint(x.shape)\nprint(x.dtype)\n\nndarray = np.ones([3, 3])\n\nprint(&quot;TensorFlow operations convert numpy arrays to Tensors automatically&quot;)\ntensor = tf.multiply(ndarray, 42)\nprint(tensor)\n\nprint(&quot;And NumPy operations convert Tensors to numpy arrays automatically&quot;)\nprint(np.add(tensor, 1))\n\nprint(&quot;The .numpy() method explicitly converts a Tensor to a numpy array&quot;)\nprint(tensor.numpy())</code></pre></div>\n<h3 id=\"tensorflow-save-on-gpu\"><a href=\"#tensorflow-save-on-gpu\" aria-label=\"tensorflow save on gpu permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tensorflow save on GPU</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">x = tf.random_uniform([3, 3])\n\nprint(&quot;Is there a GPU available: &quot;),\nprint(tf.test.is_gpu_available())\n\nprint(&quot;Is the Tensor on GPU #0:  &quot;),\nprint(x.device.endswith(&#39;GPU:0&#39;))</code></pre></div>\n<h3 id=\"a-small-script-to-show-a-custom-training-using\"><a href=\"#a-small-script-to-show-a-custom-training-using\" aria-label=\"a small script to show a custom training using permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>A small script to show a custom training using</h3>\n<p>*class,  assert, plot<br>\n*variable, gradient, and loss</p>\n<p>\"\"\"# Custom training: basics\n\"\"\"</p>\n<p>from <strong>future</strong> import absolute<em>import, division, print</em>function, unicode_literals</p>\n<p>import tensorflow as tf</p>\n<p>tf.enable<em>eager</em>execution()</p>\n<p>\"\"\"## Variables</p>\n<p>Tensors in TensorFlow are immutable stateless objects. Machine learning models, however, need to have changing state: as your model trains, the same code to compute predictions should behave differently over time (hopefully with a lower loss!). To represent this state which needs to change over the course of your computation, you can choose to rely on the fact that Python is a stateful programming language:\n\"\"\"</p>\n<h1 id=\"using-python-state\"><a href=\"#using-python-state\" aria-label=\"using python state permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Using python state</h1>\n<p>x = tf.zeros([10, 10])\nx += 2  # This is equivalent to x = x + 2, which does not mutate the original\n# value of x\nprint(x)</p>\n<p>\"\"\"TensorFlow, however, has stateful operations built in, and these are often more pleasant to use than low-level Python representations of your state. To represent weights in a model, for example, it's often convenient and efficient to use TensorFlow variables.</p>\n<p>A Variable is an object which stores a value and, when used in a TensorFlow computation, will implicitly read from this stored value. There are operations (<code class=\"language-text\">tf.assign_sub</code>, <code class=\"language-text\">tf.scatter_update</code>, etc) which manipulate the value stored in a TensorFlow variable.\n\"\"\"</p>\n<p>v = tf.Variable(1.0)\nassert v.numpy() == 1.0</p>\n<h1 id=\"re-assign-the-value\"><a href=\"#re-assign-the-value\" aria-label=\"re assign the value permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Re-assign the value</h1>\n<p>v.assign(3.0)\nassert v.numpy() == 3.0</p>\n<h1 id=\"use-v-in-a-tensorflow-operation-like-tfsquare-and-reassign\"><a href=\"#use-v-in-a-tensorflow-operation-like-tfsquare-and-reassign\" aria-label=\"use v in a tensorflow operation like tfsquare and reassign permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Use <code class=\"language-text\">v</code> in a TensorFlow operation like tf.square() and reassign</h1>\n<p>v.assign(tf.square(v))\nassert v.numpy() == 9.0</p>\n<p>\"\"\"Computations using Variables are automatically traced when computing gradients. For Variables representing embeddings TensorFlow will do sparse updates by default, which are more computation and memory efficient.</p>\n<p>Using Variables is also a way to quickly let a reader of your code know that this piece of state is mutable.</p>\n<h2 id=\"example-fitting-a-linear-model\"><a href=\"#example-fitting-a-linear-model\" aria-label=\"example fitting a linear model permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example: Fitting a linear model</h2>\n<p>Let's now put the few concepts we have so far ---<code class=\"language-text\">Tensor</code>, <code class=\"language-text\">GradientTape</code>, <code class=\"language-text\">Variable</code> --- to build and train a simple model. This typically involves a few steps:</p>\n<ol>\n<li>Define the model.</li>\n<li>Define a loss function.</li>\n<li>Obtain training data.</li>\n<li>Run through the training data and use an \"optimizer\" to adjust the variables to fit the data.</li>\n</ol>\n<p>In this tutorial, we'll walk through a trivial example of a simple linear model: <code class=\"language-text\">f(x) = x * W + b</code>, which has two variables - <code class=\"language-text\">W</code> and <code class=\"language-text\">b</code>. Furthermore, we'll synthesize data such that a well trained model would have <code class=\"language-text\">W = 3.0</code> and <code class=\"language-text\">b = 2.0</code>.</p>\n<h3 id=\"define-the-model\"><a href=\"#define-the-model\" aria-label=\"define the model permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Define the model</h3>\n<p>Let's define a simple class to encapsulate the variables and the computation.\n\"\"\"</p>\n<p>class Model(object):\ndef <strong>init</strong>(self):\n# Initialize variable to (5.0, 0.0)\n# In practice, these should be initialized to random values.\nself.W = tf.Variable(5.0)\nself.b = tf.Variable(0.0)</p>\n<p>  def <strong>call</strong>(self, x):\nreturn self.W * x + self.b</p>\n<p>model = Model()</p>\n<p>assert model(3.0).numpy() == 15.0</p>\n<p>\"\"\"### Define a loss function</p>\n<p>A loss function measures how well the output of a model for a given input matches the desired output. Let's use the standard L2 loss.\n\"\"\"</p>\n<p>def loss(predicted<em>y, desired</em>y):\nreturn tf.reduce<em>mean(tf.square(predicted</em>y - desired_y))</p>\n<p>\"\"\"### Obtain training data</p>\n<p>Let's synthesize the training data with some noise.\n\"\"\"</p>\n<p>TRUE<em>W = 3.0\nTRUE</em>b = 2.0\nNUM_EXAMPLES = 1000</p>\n<p>inputs  = tf.random<em>normal(shape=[NUM</em>EXAMPLES])\nnoise   = tf.random<em>normal(shape=[NUM</em>EXAMPLES])\noutputs = inputs * TRUE<em>W + TRUE</em>b + noise</p>\n<p>\"\"\"Before we train the model let's visualize where the model stands right now. We'll plot the model's predictions in red and the training data in blue.\"\"\"</p>\n<p>import matplotlib.pyplot as plt</p>\n<p>plt.scatter(inputs, outputs, c='b')\nplt.scatter(inputs, model(inputs), c='r')\nplt.show()</p>\n<p>print('Current loss: '),\nprint(loss(model(inputs), outputs).numpy())</p>\n<p>\"\"\"### Define a training loop</p>\n<p>We now have our network and our training data. Let's train it, i.e., use the training data to update the model's variables (<code class=\"language-text\">W</code> and <code class=\"language-text\">b</code>) so that the loss goes down using <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">gradient descent</a>. There are many variants of the gradient descent scheme that are captured in <code class=\"language-text\">tf.train.Optimizer</code> implementations. We'd highly recommend using those implementations, but in the spirit of building from first principles, in this particular example we will implement the basic math ourselves.\n\"\"\"</p>\n<p>def train(model, inputs, outputs, learning<em>rate):\nwith tf.GradientTape() as t:\ncurrent</em>loss = loss(model(inputs), outputs)\ndW, db = t.gradient(current<em>loss, [model.W, model.b])\nmodel.W.assign</em>sub(learning<em>rate * dW)\nmodel.b.assign</em>sub(learning_rate * db)</p>\n<p>\"\"\"Finally, let's repeatedly run through the training data and see how <code class=\"language-text\">W</code> and <code class=\"language-text\">b</code> evolve.\"\"\"</p>\n<p>model = Model()</p>\n<h1 id=\"collect-the-history-of-w-values-and-b-values-to-plot-later\"><a href=\"#collect-the-history-of-w-values-and-b-values-to-plot-later\" aria-label=\"collect the history of w values and b values to plot later permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Collect the history of W-values and b-values to plot later</h1>\n<p>Ws, bs = [], []\nepochs = range(10)\nfor epoch in epochs:\nWs.append(model.W.numpy())\nbs.append(model.b.numpy())\ncurrent_loss = loss(model(inputs), outputs)</p>\n<p>  train(model, inputs, outputs, learning<em>rate=0.1)\nprint('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n(epoch, Ws[-1], bs[-1], current</em>loss))</p>\n<h1 id=\"lets-plot-it-all\"><a href=\"#lets-plot-it-all\" aria-label=\"lets plot it all permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Let's plot it all</h1>\n<p>plt.plot(epochs, Ws, 'r',\nepochs, bs, 'b')\nplt.plot([TRUE_W] * len(epochs), 'r--',\n[TRUE_b] * len(epochs), 'b--')\nplt.legend(['W', 'b', 'true W', 'true_b'])\nplt.show()</p>\n<p>\"\"\"## Next Steps</p>\n<p>In this tutorial we covered <code class=\"language-text\">Variable</code>s and built and trained a simple linear model using the TensorFlow primitives discussed so far.</p>\n<p>In theory, this is pretty much all you need to use TensorFlow for your machine learning research.\nIn practice, particularly for neural networks, the higher level APIs like <code class=\"language-text\">tf.keras</code> will be much more convenient since it provides higher level building blocks (called \"layers\"), utilities to save and restore state, a suite of loss functions, a suite of optimization strategies etc.\n\"\"\"</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"></code></pre></div>","fields":{"slug":"/python/tensorflowpart1/"},"timeToRead":6,"excerpt":"Basic Operationsinitilize tensors(10,10) array with zero elementstensors basic operationstensorflow save on GPUA small script to show a…","fileAbsolutePath":"/Users/zhangh/opt/classes/website1/docs/content/python/tensorflowpart1.md"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/python/tensorflowpart1/"}}