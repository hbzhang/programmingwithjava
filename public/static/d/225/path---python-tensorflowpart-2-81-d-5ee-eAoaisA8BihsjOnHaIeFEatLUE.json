{"data":{"site":{"siteMetadata":{"siteUrl":"http://ghost.local:8000","title":"Ghost Docs","description":"Everything you need to know about working with the Ghost professional publishing platform."}},"markdownRemark":{"frontmatter":{"title":"Tensorflow in Python Part II","date":"2019-05-21","date_pretty":"21 May, 2019","path":null,"meta_title":"Python Data Structure","meta_description":"This tutorial is used for going through python data structure","image":null,"next":null,"sidebar":null,"toc":null,"keywords":["setup","production","server","ubuntu"]},"html":"<h2 id=\"basic-operations\"><a href=\"#basic-operations\" aria-label=\"basic operations permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Basic Operations</h2>\n<h3 id=\"multilayer-perceptron-mlp-for-multi-class-softmax-classification\"><a href=\"#multilayer-perceptron-mlp-for-multi-class-softmax-classification\" aria-label=\"multilayer perceptron mlp for multi class softmax classification permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multilayer Perceptron (MLP) for multi-class softmax classification</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nimport numpy as np\nx_train = np.random.random((1000, 20))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\nx_test = np.random.random((100, 20))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\n\nmodel = Sequential()\n# Dense(64) is a fully-connected layer with 64 hidden units.\n# in the first layer, you must specify the expected input data shape:\n# here, 20-dimensional vectors.\nmodel.add(Dense(64, activation=&#39;relu&#39;, input_dim=20))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation=&#39;relu&#39;))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation=&#39;softmax&#39;))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=&#39;categorical_crossentropy&#39;,\n              optimizer=sgd,\n              metrics=[&#39;accuracy&#39;])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)</code></pre></div>\n<h3 id=\"mlp-for-binary-classification\"><a href=\"#mlp-for-binary-classification\" aria-label=\"mlp for binary classification permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>MLP for binary classification</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n# Generate dummy data\nx_train = np.random.random((1000, 20))\ny_train = np.random.randint(2, size=(1000, 1))\nx_test = np.random.random((100, 20))\ny_test = np.random.randint(2, size=(100, 1))\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, activation=&#39;relu&#39;))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation=&#39;relu&#39;))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=&#39;sigmoid&#39;))\n\nmodel.compile(loss=&#39;binary_crossentropy&#39;,\n              optimizer=&#39;rmsprop&#39;,\n              metrics=[&#39;accuracy&#39;])\n\nmodel.fit(x_train, y_train,\n          epochs=20,\n          batch_size=128)\nscore = model.evaluate(x_test, y_test, batch_size=128)</code></pre></div>\n<h3 id=\"a-simplest-training-using-mnist-dataset\"><a href=\"#a-simplest-training-using-mnist-dataset\" aria-label=\"a simplest training using mnist dataset permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>A simplest training using mnist dataset</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import tensorflow as tf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer=&#39;adam&#39;,\n              loss=&#39;sparse_categorical_crossentropy&#39;,\n              metrics=[&#39;accuracy&#39;])\n\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)</code></pre></div>\n<a class=\"sbox\" href=\"https://www.digitalocean.com/docs/one-clicks/ghost/\" target=\"_blank\" rel=\"noopener\">\n    <div class=\"sbox-image\">\n        <svg id=\"Layer_1\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"65.2 173.5 180 180\"><style>.st0{fill:#0080ff}</style><g id=\"XMLID_229_\"><g id=\"XMLID_690_\"><g id=\"XMLID_691_\"><g id=\"XMLID_44_\"><g id=\"XMLID_48_\"><path id=\"XMLID_49_\" class=\"st0\" d=\"M155.2 351.7v-34.2c36.2 0 64.3-35.9 50.4-74-5.1-14.1-16.4-25.4-30.5-30.5-38.1-13.8-74 14.2-74 50.4H67c0-57.7 55.8-102.7 116.3-83.8 26.4 8.3 47.5 29.3 55.7 55.7 18.9 60.6-26 116.4-83.8 116.4z\"/></g><path id=\"XMLID_47_\" class=\"st0\" d=\"M155.3 317.6h-34v-34h34z\"/><path id=\"XMLID_46_\" class=\"st0\" d=\"M121.3 343.8H95.1v-26.2h26.2z\"/><path id=\"XMLID_45_\" class=\"st0\" d=\"M95.1 317.6H73.2v-21.9h21.9v21.9z\"/></g></g></g></g></svg>\n    </div>\n    <div class=\"sbox-content\">\n        <h4> Dense layer is the layer refering to every node of neuron is connected to a previous layer. &#x1F449;</h4>\n        <h4> Dense(512) is a fully-connected layer with 512 hidden units. &#x1F604; </h4>\n        <img src=\"https://www.researchgate.net/profile/Igor_Gilitschenski/publication/311920717/figure/fig2/AS:617625239425024@1524264740078/Sample-dense-neural-network-with-2-fully-connected-layers-2-dropout-layers-and-a_Q320.jpg\" alt=\"Smiley face\" height=\"142\" width=\"142\">    \n    </div>\n</a>\n<h3 id=\"a-more--complete-training-using-mnist-dataset\"><a href=\"#a-more--complete-training-using-mnist-dataset\" aria-label=\"a more  complete training using mnist dataset permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>A more  complete training using mnist dataset</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == &#39;channels_first&#39;:\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype(&#39;float32&#39;)\nx_test = x_test.astype(&#39;float32&#39;)\nx_train /= 255\nx_test /= 255\nprint(&#39;x_train shape:&#39;, x_train.shape)\nprint(x_train.shape[0], &#39;train samples&#39;)\nprint(x_test.shape[0], &#39;test samples&#39;)\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation=&#39;relu&#39;,\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation=&#39;relu&#39;))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation=&#39;softmax&#39;))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=[&#39;accuracy&#39;])\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(&#39;Test loss:&#39;, score[0])\nprint(&#39;Test accuracy:&#39;, score[1])</code></pre></div>\n<h3 id=\"vgg-like-convnet\"><a href=\"#vgg-like-convnet\" aria-label=\"vgg like convnet permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>VGG-like convnet:</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">import numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nx_train = np.random.random((100, 100, 100, 3))\ny_train = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10)\nx_test = np.random.random((20, 100, 100, 3))\ny_test = keras.utils.to_categorical(np.random.randint(10, size=(20, 1)), num_classes=10)\n\nmodel = Sequential()\n# input: 100x100 images with 3 channels -&gt; (100, 100, 3) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(100, 100, 3)))\nmodel.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;))\nmodel.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=&#39;relu&#39;))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation=&#39;softmax&#39;))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=sgd)\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=32)</code></pre></div>\n<h3 id=\"custom_training-\"><a href=\"#custom_training-\" aria-label=\"custom_training  permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>custom_training 👋</h3>\n<h3 id=\"sequence-classification-with-lstm-\"><a href=\"#sequence-classification-with-lstm-\" aria-label=\"sequence classification with lstm  permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Sequence classification with LSTM: 👋</h3>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\n\nmax_features = 1024\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, output_dim=256))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=&#39;sigmoid&#39;))\n\nmodel.compile(loss=&#39;binary_crossentropy&#39;,\n              optimizer=&#39;rmsprop&#39;,\n              metrics=[&#39;accuracy&#39;])\n\nmodel.fit(x_train, y_train, batch_size=16, epochs=10)\nscore = model.evaluate(x_test, y_test, batch_size=16)</code></pre></div>","fields":{"slug":"/python/tensorflowpart2/"},"timeToRead":5,"excerpt":"Basic OperationsMultilayer Perceptron (MLP) for multi-class softmax classificationMLP for binary classificationA simplest training using…","fileAbsolutePath":"/Users/zhangh/opt/classes/website1/docs/content/python/tensorflowpart2.md"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/python/tensorflowpart2/"}}